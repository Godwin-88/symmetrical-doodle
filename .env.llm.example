# LLM and RAG Configuration
# Copy this file to .env and configure your settings

# ============================================================================
# LOCAL LLM CONFIGURATION
# ============================================================================

# Enable/disable local LLM
LLM__LOCAL_LLM__ENABLED=true

# Local model path (download models to this directory)
LLM__LOCAL_LLM__MODEL_PATH=./models/llama-3.1-8b-instruct

# Model type: llama, mistral, qwen, phi
LLM__LOCAL_LLM__MODEL_TYPE=llama

# Device: cpu, cuda, mps
LLM__LOCAL_LLM__DEVICE=cuda

# Quantization: 4bit, 8bit, or leave empty for no quantization
LLM__LOCAL_LLM__QUANTIZATION=4bit

# GPU memory fraction (0.1 to 1.0)
LLM__LOCAL_LLM__GPU_MEMORY_FRACTION=0.8

# Generation settings
LLM__LOCAL_LLM__MAX_TOKENS=4096
LLM__LOCAL_LLM__TEMPERATURE=0.1
LLM__LOCAL_LLM__CONTEXT_LENGTH=8192

# ============================================================================
# EXTERNAL LLM API CONFIGURATION
# ============================================================================

# Enable/disable external LLM APIs
LLM__EXTERNAL_LLM__ENABLED=true

# Primary provider: openai, anthropic, groq, together
LLM__EXTERNAL_LLM__PRIMARY_PROVIDER=groq

# API Keys (get these from respective providers)
OPENAI_API_KEY=your_openai_api_key_here
ANTHROPIC_API_KEY=your_anthropic_api_key_here
GROQ_API_KEY=your_groq_api_key_here
TOGETHER_API_KEY=your_together_api_key_here

# Model configurations
LLM__EXTERNAL_LLM__OPENAI_MODEL=gpt-4o-mini
LLM__EXTERNAL_LLM__ANTHROPIC_MODEL=claude-3-haiku-20240307
LLM__EXTERNAL_LLM__GROQ_MODEL=llama-3.1-8b-instant
LLM__EXTERNAL_LLM__TOGETHER_MODEL=meta-llama/Llama-3-8b-chat-hf

# Generation settings
LLM__EXTERNAL_LLM__MAX_TOKENS=4096
LLM__EXTERNAL_LLM__TEMPERATURE=0.1
LLM__EXTERNAL_LLM__TIMEOUT=30

# ============================================================================
# RAG CONFIGURATION
# ============================================================================

# Enable/disable RAG
LLM__RAG__ENABLED=true

# Vector database: chroma, pinecone, weaviate, pgvector
LLM__RAG__VECTOR_DB=chroma
LLM__RAG__VECTOR_DB_PATH=./data/vector_db

# Embedding model
LLM__RAG__EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
LLM__RAG__EMBEDDING_DEVICE=cuda

# Document processing
LLM__RAG__CHUNK_SIZE=1000
LLM__RAG__CHUNK_OVERLAP=200
LLM__RAG__TOP_K_RETRIEVAL=5
LLM__RAG__SIMILARITY_THRESHOLD=0.7
LLM__RAG__MAX_FILE_SIZE_MB=50

# ============================================================================
# RESEARCH CONFIGURATION
# ============================================================================

# Enable research capabilities
LLM__RESEARCH__ENABLED=true
LLM__RESEARCH__ENABLE_WEB_SEARCH=true
LLM__RESEARCH__ENABLE_DOCUMENT_ANALYSIS=true
LLM__RESEARCH__ENABLE_SENTIMENT_ANALYSIS=true

# Web search settings
LLM__RESEARCH__MAX_SEARCH_RESULTS=10

# Sentiment analysis model
LLM__RESEARCH__SENTIMENT_MODEL=finbert

# ============================================================================
# PERFORMANCE SETTINGS
# ============================================================================

# Concurrent requests
LLM__MAX_CONCURRENT_REQUESTS=10
LLM__REQUEST_TIMEOUT=60
LLM__CACHE_TTL=3600

# ============================================================================
# SYSTEM PROMPTS
# ============================================================================

LLM__SYSTEM_PROMPT_FINANCIAL="You are an expert financial analyst and quantitative researcher with deep knowledge of financial markets, instruments, and trading strategies. Provide accurate, data-driven insights with proper risk disclaimers. Always cite sources when available."

LLM__SYSTEM_PROMPT_RESEARCH="You are a financial research assistant specializing in market analysis and trend identification. Provide comprehensive, well-structured analysis with clear reasoning and evidence."