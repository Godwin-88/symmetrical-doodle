"""
Local ZIP File Ingestion Test

Tests the complete pipeline using local ZIP file instead of Google Drive.
Processes Taleb books from the local ZIP archive.
"""

import asyncio
import os
import sys
from pathlib import Path

# Add current directory to path for imports
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from services.local_zip_discovery import LocalZipDiscoveryService
from services.inventory_report_generator import InventoryReportGenerator
from services.quality_audit_service import QualityAuditor
from services.supabase_storage import SupabaseStorageService
from services.simple_pdf_parser import SimplePDFParser
from services.simple_chunker import SimpleChunker
from services.embedding_service import EmbeddingService
from core.config import load_config
from core.logging import configure_logging, get_logger


async def test_local_zip_pipeline():
    """Test the complete pipeline with local ZIP file"""
    
    # Configure logging
    configure_logging()
    logger = get_logger(__name__)
    
    logger.info("Starting local ZIP ingestion test")
    
    try:
        # Load configuration
        settings = load_config("development")
        
        # Check if local ZIP is configured
        if not settings.local_zip.use_local_zip:
            logger.error("Local ZIP processing not enabled. Set USE_LOCAL_ZIP=true in .env")
            return False
        
        if not settings.local_zip.zip_path:
            logger.error("No ZIP path configured. Please set LOCAL_ZIP_PATH in .env")
            return False
        
        zip_path = Path(settings.local_zip.zip_path)
        if not zip_path.exists():
            logger.error(f"ZIP file not found: {zip_path}")
            return False
        
        logger.info(f"Processing ZIP file: {zip_path}")
        
        # Step 1: Initialize Supabase Storage
        logger.info("Step 1: Initializing Supabase storage")
        storage_service = SupabaseStorageService(settings.supabase)
        
        schema_success = await storage_service.initialize_client()
        if not schema_success:
            logger.error("Failed to initialize Supabase client")
            return False
        
        logger.info("‚úÖ Supabase client initialized")
        
        # Step 2: Discover PDFs in ZIP
        logger.info("Step 2: Discovering PDFs in ZIP file")
        discovery_service = LocalZipDiscoveryService()
        
        discovery_result = await discovery_service.discover_pdfs_from_zip(
            str(zip_path),
            settings.local_zip.extract_path
        )
        
        if not discovery_result.success:
            logger.error("PDF discovery failed")
            for error in discovery_result.errors:
                logger.error(f"Discovery error: {error}")
            return False
        
        logger.info(f"‚úÖ Discovery completed: {len(discovery_result.pdfs_found)} PDFs found")
        logger.info(f"   - Total files scanned: {discovery_result.total_files_scanned}")
        logger.info(f"   - Inaccessible files: {len(discovery_result.inaccessible_files)}")
        
        # Print discovered PDFs
        logger.info("Discovered PDFs:")
        for pdf in discovery_result.pdfs_found:
            logger.info(f"   - {pdf.name} ({pdf.domain_classification})")
        
        # Step 3: Generate Inventory Report
        logger.info("Step 3: Generating inventory report")
        inventory_generator = InventoryReportGenerator()
        
        inventory_report = await inventory_generator.generate_inventory_report(
            discovery_result,
            include_raw_data=True
        )
        
        logger.info("‚úÖ Inventory report generated")
        logger.info(f"   - Total PDFs: {inventory_report.total_pdfs_found}")
        logger.info(f"   - Domains found: {len(inventory_report.domain_stats)}")
        logger.info(f"   - Accessibility rate: {inventory_report.accessibility_stats.accessibility_rate:.1f}%")
        
        # Print domain distribution
        logger.info("Domain Distribution:")
        for stat in inventory_report.domain_stats:
            logger.info(f"   - {stat.category.value}: {stat.count} files ({stat.percentage:.1f}%)")
        
        # Step 4: Process a sample PDF (first one)
        if discovery_result.pdfs_found:
            logger.info("Step 4: Processing sample PDF")
            sample_pdf = discovery_result.pdfs_found[0]
            
            # Parse PDF
            pdf_parser = SimplePDFParser()
            parsed_doc = await pdf_parser.parse_pdf_from_file(sample_pdf.local_path)
            
            if parsed_doc:
                logger.info(f"‚úÖ PDF parsed: {len(parsed_doc.content)} characters")
                
                # Chunk content
                chunker = SimpleChunker()
                chunks = await chunker.chunk_document(parsed_doc)
                
                logger.info(f"‚úÖ Content chunked: {len(chunks)} chunks")
                
                # Store document in Supabase
                from services.supabase_storage import DocumentMetadata, ProcessingStatus
                
                document_metadata = DocumentMetadata(
                    file_id=sample_pdf.file_id,
                    title=sample_pdf.name,
                    source_url=sample_pdf.web_view_link,
                    content=parsed_doc.content[:5000],  # Limit for demo
                    parsing_method=parsed_doc.parsing_method,
                    domain_classification=sample_pdf.domain_classification,
                    processing_status=ProcessingStatus.COMPLETED,
                    file_size_bytes=sample_pdf.size
                )
                
                storage_result = await storage_service.store_document(document_metadata)
                
                if storage_result.success:
                    document_id = storage_result.record_id
                    logger.info(f"‚úÖ Document stored in Supabase: {document_id}")
                    
                    # Store chunks
                    from services.supabase_storage import ChunkData, EmbeddedChunk
                    
                    embedded_chunks = []
                    for i, chunk in enumerate(chunks[:5]):  # Limit to first 5 chunks for demo
                        chunk_data = ChunkData(
                            document_id=document_id,
                            content=chunk.content,
                            chunk_order=i,
                            section_header=chunk.section_header if hasattr(chunk, 'section_header') else None,
                            token_count=chunk.token_count
                        )
                        
                        # Create embedded chunk with dummy embedding for demo
                        embedded_chunk = EmbeddedChunk(
                            chunk_data=chunk_data,
                            embedding_vector=[0.0] * 1536  # Dummy embedding vector
                        )
                        embedded_chunks.append(embedded_chunk)
                    
                    chunk_results = await storage_service.store_chunks(embedded_chunks)
                    successful_chunks = [r for r in chunk_results if r.success]
                    logger.info(f"‚úÖ {len(successful_chunks)} chunks stored in Supabase")
                else:
                    logger.warning(f"Failed to store document in Supabase: {storage_result.error_message}")
            else:
                logger.warning("Failed to parse sample PDF")
        
        # Step 5: Quality Audit
        logger.info("Step 5: Conducting quality audit")
        
        # Create mock documents for audit (in real scenario, these would come from Supabase)
        mock_documents = []
        for pdf in discovery_result.pdfs_found[:3]:  # Sample first 3 PDFs
            mock_documents.append({
                'document_id': pdf.file_id,
                'name': pdf.name,
                'domain_classification': pdf.domain_classification or 'Unknown',
                'content': f"Mock content for {pdf.name}. This represents the extracted text content from the PDF. " * 20
            })
        
        auditor = QualityAuditor()
        audit_report = await auditor.conduct_quality_audit(mock_documents)
        
        logger.info("‚úÖ Quality audit completed")
        logger.info(f"   - Samples collected: {audit_report.total_samples_collected}")
        logger.info(f"   - Overall quality: {audit_report.overall_quality_level.value}")
        
        # Step 6: Save Reports
        output_dir = Path("test_outputs")
        output_dir.mkdir(exist_ok=True)
        
        # Save inventory report
        inventory_path = output_dir / "local_zip_inventory_report.json"
        await inventory_generator.save_report_to_file(inventory_report, inventory_path)
        
        # Save audit report
        audit_path = output_dir / "local_zip_quality_audit.json"
        await auditor.save_audit_report(audit_report, audit_path)
        
        # Generate human-readable summary
        summary = await inventory_generator.generate_summary_report(inventory_report)
        summary_path = output_dir / "local_zip_inventory_summary.txt"
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write(summary)
        
        logger.info(f"‚úÖ Reports saved to {output_dir}")
        
        # Step 7: Print Final Summary
        print("\n" + "="*80)
        print("LOCAL ZIP INGESTION TEST RESULTS")
        print("="*80)
        print(f"‚úÖ ZIP File Processing: SUCCESS")
        print(f"‚úÖ PDF Discovery: {len(discovery_result.pdfs_found)} PDFs found")
        print(f"‚úÖ Supabase Storage: Schema initialized and sample data stored")
        print(f"‚úÖ Inventory Report: Generated with {len(inventory_report.domain_stats)} domains")
        print(f"‚úÖ Quality Audit: Completed with realistic data")
        print(f"‚úÖ Reports saved to: {output_dir.absolute()}")
        
        print(f"\nüìö Discovered Books:")
        for pdf in discovery_result.pdfs_found:
            print(f"   - {pdf.name}")
            print(f"     Domain: {pdf.domain_classification}")
            print(f"     Size: {pdf.size / (1024*1024):.1f} MB")
        
        if inventory_report.potential_issues:
            print(f"\n‚ö†Ô∏è  Potential Issues Identified:")
            for issue in inventory_report.potential_issues:
                print(f"   - {issue}")
        
        print(f"\nüìä Key Metrics:")
        print(f"   - Total PDFs: {inventory_report.total_pdfs_found}")
        print(f"   - Total Size: {inventory_report.total_size_mb:.1f} MB")
        print(f"   - Accessibility Rate: {inventory_report.accessibility_stats.accessibility_rate:.1f}%")
        print(f"   - Estimated Processing Time: {inventory_report.estimated_processing_time_hours:.1f} hours")
        
        print(f"\nüéØ Next Steps:")
        print(f"   1. Review generated reports in {output_dir}/")
        print(f"   2. Process all PDFs through the complete pipeline")
        print(f"   3. Generate embeddings for semantic search")
        print(f"   4. Set up automated ingestion workflow")
        
        return True
        
    except Exception as e:
        logger.error(f"Test failed with error: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


async def main():
    """Main test function"""
    success = await test_local_zip_pipeline()
    
    if success:
        print("\nüéâ Local ZIP ingestion test completed successfully!")
        print("\nThe system is now ready to process your Taleb books collection!")
    else:
        print("\n‚ùå Test failed. Please check the logs and configuration.")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())